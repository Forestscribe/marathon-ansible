- vars:
    logstash_conf_path: "/etc/logstash/logstash.conf"
    logstash_conf_url: "{{ zk_http_url }}/{{ logstash_conf_path }}"

  block:
    - name: upload logstash conf in zk
      znode:
        hosts: "{{ zk_host }}"
        name: "{{ logstash_conf_path }}"
        value: "{{ lookup('template', 'logstash/logstash.conf') }}"
        state: present

    - name: Deploy logging
      marathon:
        marathon_uri: "{{ marathon_uri }}"
        group_id: core/logging
        group_json:
            dependencies:
                - /core/lb
            apps:
                -   id: "elasticsearch"
                    cmd:
                    cpus: 4
                    mem: 4096  # at least 2GB is required!
                    disk: 0
                    instances: 1
                    container:
                      type: DOCKER
                      volumes:
                      - containerPath: "elasticsearch"
                        mode: "RW"
                        persistent:
                         type: "root"
                         size: 10000
                      - containerPath: "/usr/share/elasticsearch/data"
                        hostPath: "elasticsearch"
                        mode: "RW"
                      docker:
                        image:  "elasticsearch:alpine"
                        network: BRIDGE
                        privileged: false
                        parameters: []
                        forcePullImage: false
                        portMappings:
                        - containerPort: 9200
                          servicePort: "{{ servicePorts['elasticsearch'] }}"
                          protocol: tcp

                    labels:
                      HAPROXY_GROUP: external
                      HAPROXY_BIND_ADDR: 172.17.0.1
                      subdomain: '/elasticsearch/'
                      upstream_name: 'elasticsearch'

                    env:
                        PORT: "9200"

                    residency:
                        taskLostBehavior: "WAIT_FOREVER"
                    upgradeStrategy:
                        minimumHealthCapacity: 0.5
                        maximumOverCapacity: 0
                    healthChecks:
                    - "{{ basic_http_health_check }}"
                -   id: "kibana"
                    cmd:
                    cpus: 4
                    mem: 4096
                    disk: 0
                    instances: 1
                    container:
                      type: DOCKER
                      docker:
                        image:  "kibana:5"
                        network: BRIDGE
                        privileged: false
                        parameters: []
                        forcePullImage: false
                        portMappings:
                        - containerPort: 5601
                          servicePort: "{{ servicePorts.kibana }}"
                          protocol: tcp

                    labels:
                      HAPROXY_GROUP: external
                      HAPROXY_BIND_ADDR: 172.17.0.1
                      subdomain: 'kibana'
                      upstream_name: 'kibana'

                    env:
                        PORT: "5601"
                        ELASTICSEARCH_URL: "http://{{docker_ip}}:{{ servicePorts.elasticsearch }}"
                    healthChecks:
                    - "{{ basic_http_health_check }}"

                -   id: "logstash"
                    cpus: 1
                    mem: 1024
                    instances: "{{marathon_num_agents}}"
                    constraints:
                    - [ "hostname", "UNIQUE" ]
                    container:
                      type: DOCKER
                      docker:
                        image: logstash:5
                        network: HOST
                    args: ["-f", "/mnt/mesos/sandbox/logstash.conf"]
                    uris:
                        - "{{ logstash_conf_url }}"
                    upgradeStrategy:
                        minimumHealthCapacity: 0.5
                        maximumOverCapacity: 0
